\section{Related Work}
\label{sec:related}

%% \begin{comment}
%% We first touch on two different approaches for analyzing cryptographic
%% protocols: the symbolic approach and the computational approach. Their
%% distinctions show up in our subsequent discussions of related process calculi,
%% tools for cryptographic reasoning, and variations of the UC framework.

%% \subsection{Symbolic vs. Computational Cryptography}
%% \label{subsec:symbolic-computational}

%% Approaches for reasoning about the security of cryptographic protocols generally
%% operate in one of two models---the symbolic model or the computational model---each
%% of which have their merits and drawbacks.

%% In the symbolic model, cryptographic operations are abstracted (and idealized)
%% as term algebras, and adversary capabilities are defined as deduction rules over
%% these terms (which can be applied
%% nondeterministically)~\cite{cortier2011survey}. Viewing protocols from this
%% abstract vantage point makes the analysis relatively tractable. However, because
%% adversaries are nondeterministic, their capabilities must be severely restricted
%% and it is therefore possible to verify protocols that are susceptible to attacks
%% that lie outside the adversary model.

%% In the computational model, cryptographic operations are modeled as functions
%% over bitstrings, and adversaries are modeled as probabilistic polynomial time
%% Turing machines. Here, security is defined in terms of the probability and
%% computational complexity of attacks, which complicates proofs, but admits much
%% stronger security guarantees. Analyzing protocols in the computational model is
%% the modus operandi in cryptography, and, for the most part, proofs are done
%% manually. The goal of this work, and others we discuss later, seek to change
%% this dynamic.

%% There is also a body of interesting work on computational soundness, which
%% focuses on bridging the gap between the symbolic and computational
%% models~\cite{abadi2002reconciling}.
%% \end{comment}

\subsection{Process Calculi}
\label{subsec:process-calculi}

Process calculi have a long and rich history. ILC occupies a point in this space
that is particularly suited to faithfully capturing interactive Turing machines
(and hence, computational cryptography), but plenty of existing calculi are also
cryptographically-flavored and/or enjoy similar properties to ILC. We survey
some of them here.

\paragraph{With symbolic semantics.} Two of the early adaptations of process calculi for reasoning about
cryptographic protocols were the spi calculus~\cite{abadi1999calculus} and the
applied $\pi$-calculus~\cite{abadi2001mobile}, both of which extend the standard
$\pi$-calculus with cryptographic
operations~\cite{milner1999communicating}. Importantly, they introduced
techniques for proving (symbolic) security via a notion of observational
equivalence.

\paragraph{With computational semantics.}
Naturally, ensuing work has turned to bridging the gap between this PL-style of
formalization and the computational model of cryptography by outfitting these
calculi with a computational semantics.

%The spi calculus extends the
%$\pi$-calculus with constructs for encryption and decryption, and introduces a
%technique for proving authenticity and secrecy properties of cryptographic
%protocols via observational equivalence between spi calculus processes. The
%applied $\pi$-calculus builds on this work, permitting a wider variety of
%cryptographic primitives that can be defined by means of an equational theory.

%A variety of tools for protocol analysis, some of which we discuss in the next
%subsection, can trace their roots back to these works. They generally operate in
%what is called the \emph{symbolic} model of cryptography (also known as the
%Dolev-Yao model), in which cryptographic primitives are abstracted (and
%idealized) as term algebras and adversary capabilities are defined as deduction
%rules over these terms~\cite{cortier2011survey}. Although analyzing protocols
%from this abstract vantage point makes the analysis relatively tractable, it is
%possible to verify protocols that are susceptible to simple attacks that lie
%outside the adversary model.

%A variety of tools for symbolic analysis, some of which we discuss in the next
%subsection, can trace their roots back to these works. Naturally, ensuing work
%has also turned to bridging the gap between this PL-style of formalization and
%the computational model of cryptography by outfitting existing calculi with
%a computational semantics that considers issues of complexity and probability.

Lincoln \etal~\cite{lincoln1998probabilistic} give a computational semantics to
a variant of the $\pi$-calculus, which allows one to define communicating
probabilistic polynomial-time processes; Mateus
\etal~\cite{mateus2003composition} adapts their calculus to explore (sequential)
compositionality properties in protocols.
A drawback of these protocols is that they embed probabilistic choices directly
into the definition---essentially when faced with non-determinism, each path
has equal probability.
Laud~\cite{laud2005secrecy} gives a
computational semantics to the spi calculus, which additionally includes a type
system for ensuring well-typed protocols preserve the secrecy of messages given
to it by users.

%In their work, the security of a protocol is established
%by a notion of emulation: A process $P$ emulates a process $Q$ if for any
%adversary (context) $A[\cdot]$, the trace process of $A[P]$ is observationally
%equivalent to the trace process of $B[Q]$ for ideal adversary
%$B[\cdot]$. Compositionality follows naturally: A process that uses $Q$ can also use
%$P$ as a secure implementation of $Q$.

\paragraph{With confluence.}
There are a number of other process calculi that enjoy confluence.  Berger
\etal~\cite{berger2001sequentiality} describe a type system for capturing
deterministic (sequential) computation in the $\pi$-calculus. The type system uses
affineness and stateless replication to achieve deterministic computation.
Fowler \etal~\cite{fowler2018session} present a core linear lambda calculus with
(binary) session-typed channels and exception handling that enjoys confluence
and termination.
%Confluence is due to linearity of communication endpoints, so
%communication actions on different channels can be performed in any order.
The calculus only considers two-party protocols, so for our multiparty setting,
ILC requires a sophisticated type system to achieve confluence.

\subsection{Tools for Cryptographic Analysis}
\label{subsec:tools}
There are a variety of computer-aided tools for cryptographic analysis, both
symbolic and computational. Among the symbolic tools include the NRL protocol
analyzer~\cite{meadows1996nrl}, Maude-NPA~\cite{escobar2009maude}, and
Proverif~\cite{blanchet2010proverif}. Among the computational tools include
CertiCrypt~\cite{barthe2009formal}, EasyCrypt~\cite{barthe2011computer},
CryptoVerif~\cite{blanchet2007cryptoverif}, Cryptol~\cite{lewis2003cryptol}, and
$\text{F}^{\star}$~\cite{swamy2016dependent}.
Although the computational tools do not support message-passing concurrency,
it would be interesting to compile ILC terms into EasyCrypt in order use
its tooling.

%CertiCrypt~\cite{barthe2009formal} is a framework (built on
%Coq~\cite{barras1997coq}) that supports machine-checked game-based proofs of
%security. It includes tools to reason about the equivalence of probabilistic
%programs, a relational Hoare logic, a theory of observational equivalence,
%verified program transformations, and game-based techniques.  Their experience
%shows that the type system and automated tactics provide valuable information in
%debugging proofs. EasyCrypt~\cite{barthe2011computer} is follow-up work on
%CertiCrypt, which permits more automation and shorter proof
%scripts. \todo{Details on their imperative language?}

\subsection{Variations of the UC Framework}
\label{subsec:uc-variants}
A number of variants of the UC framework have been proposed to address some of
its hitches or to make it easier to use.
%In particular, they show that certain
%aspects of the UC framework, such as ideal functionality specifications and UC
%composition, still carry over to the symbolic model.
The reactive simulatability framework (RSIM)~\cite{backes2007reactive} is
a framework for representing and analyzing cryptographic protocols in a
composable way. In contrast with UC, which uses ITMs as its computational model,
RSIM uses probabilistic IO automata, which is amenable automated reasoning tools.
In contrast to RSIM, ILC is intended to be the basis for a convenient and
flexible programming language to which we can easily port existing UC
pseudocode. 
The IITMs model~\cite{iitm,kusters2009computational} has similar
goals to ours, namely to provide a simple basis for UC.
However, our approach is simpler, for example our type system subsumes
their notion of \emph{compatibility} and \emph{connectability}, we avoid
the distinctions of \emph{net}, \emph{external}, \emph{io}, etc. tapes,
a concrete programming model.
%As one example, RSIM defines a particular
%communication model in terms of ports and schedulers, whereas in our
%case the scheduler 
%ILC leads to a programming language, more flexible and convenient to work with.

Symbolic UC~\cite{bohl2016symbolic} defines a UC-like framework
in $\pi$-calculus, but does does not aim to show computational
proofs. Instead, they prove security relative to idealized notions
of cryptography primitives. Such security claims may not transfer
to proofs based on computational hardness assumptions.

Simplified universal composability (SUC)~\cite{canetti2015simpler} gives a
simpler and restricted variant of the UC framework that is suitable for
two-party multiparty computation tasks. The main difference from vanilla UC is
that the set of parties is fixed, which greatly simplifies polynomial time
reasoning and protocol composition while maintaining the same strong properties.
We follow the SUC approach in our development of \textsf{execUC}.

\begin{comment}
\begin{enumerate}[leftmargin=*]
  \item Symbolic UC~\cite{bohl2016symbolic} transports ideas from the UC
    framework to the symbolic model of cryptography, in which cryptographic
    operations are abstracted as a term process algebra (specifically, a variant
    of the applied $\pi$-calculus) and adversary capabilities are defined by
    deduction rules over these terms. In particular, they show that certain
    aspects of the UC framework, such as ideal functionality specifications and
    UC composition, still carry over to the symbolic model. They are also able
    to show that certain results, such as the impossibility of UC commitments in
    the standard model of cryptography, can still be observed in the symbolic
    model. Although this abstract vantage point leads to simpler security proofs
    that can be amenable to automated reasoning, security guarantees derived
    from symbolic analyses are not as strong as those from computational
    analyses considered in UC and in cryptography more broadly.
  \item RSIM~\cite{backes2007reactive}.
  \item CertiCrypt~\cite{barthe2009formal} is a framework (built on
    Coq~\cite{barras1997coq}) that supports machine-checked game-based proofs of
    security. It includes tools to reason about the equivalence of probabilistic
    programs, a relational Hoare logic, a theory of observational equivalence,
    verified program transformations, and game-based techniques.  Their
    experience shows that the type system and automated tactics provide valuable
    information in debugging proofs.
  \item EasyCrypt~\cite{barthe2011computer} is follow-up work on CertiCrypt,
    which permits more automation and shorter proof scripts. \todo{Details on
      their imperative language?}
  \item ProVerif~\cite{blanchet2010proverif} is a tool for symbolically
    analyzing cryptographic protocols. It relies on the Horn theory approach, in
    which protocols and intruders are modeled as Horn theories. Protocols are
    analyzed with respect to an unbounded number of protocol sessions that may
    run concurrently and there is no bound on the number of messages an
    adversary can generate. Verifying security properties, such as secrecy,
    boils down to solving the derivation problem for Horn theories. Protocols
    can be specified as either Horn theories, or in a variant of the applied
    process calculus, which is translated into Horn theories.
  \item CryptoVerif~\cite{blanchet2007cryptoverif} works directly in the
    computational model. Produces game-based proofs valid for any number of
    sessions of the protocol in the presence of an active adversary. Games are
    represented in a process calculus inspired by the $\pi$-calculus,
    \cite{laud2005secrecy}, and \cite{mitchell2006probabilistic}. The calculus
    has a probabilistic semantics.
  \item Cryptol~\cite{lewis2003cryptol} allows for writing executable
    specifications of a protocol, which are amenable to testing, theorem
    proving, verifying equivalence to their own programs, and even generating
    code or hardware from the specification.
  \item $\text{F}^{\star}$~\cite{swamy2016dependent} is a language that functions as
    a proof assistant (SMT automation and constructive proofs using dependent
    types) as well as a general-purpose, verification-oriented programming
    language.
  \item Spi calculus~\cite{abadi1999calculus} is an extension of the
    $\pi$-calculus with cryptographic primitives. In the spi calculus, as in the
    $\pi$-calculus, channels can be passed over channels, and its scoping rules
    guarantee that an attacker cannot access a channel it is not explicitly
    given (scoping is the basis of security). The spi calculus allows expressing
    security guarantees as equivalences between spi calculus processes. For
    example, we can say that a protocol maintains the secrecy of a value $x$ by
    stating that the protocol with $x$ is equivalent to the protocol with $x'$,
    for every $x'$. Here, equivalence means equivalence in the eyes of an
    arbitrary environment that interacts with the protocol. \todo{They cannot
      take the standard bisimilarity relation as our notion of
      equivalence. Why?}

    Although equivalence makes reference to the environment, we do not need to
    give a model of the environment explicitly. Instead, the environment can be
    an arbitrary pi calculus process. In sum, their approach uses the powerful
    scoping constructs of the $\pi$-calculus, the definition of an environment as
    an arbitrary spi calculus process, and the representation of security
    properties (both integrity and security) as equivalences. However, the spi
    calculus does not include any notion of probability or complexity, so it can
    be a useful foundation for symbolic cryptography, but not computational
    cryptography.

    In $\pi$-calculus, the scope of a channel can change during a
    computation. When a process sends a restricted channel to a process outside
    the scope of the restriction, the scope is said to extrude. Why do we
    disallow extrusion in SaUCy? A central idea in spi calculus is to use
    restriction and extrusion to keep track of secret values.

    Another difference is that channels are bidirectional. 
  \item Applied $\pi$-calculus~\cite{abadi2001mobile} is a similar extension to
    the $\pi$-calculus. Here, there is no need to craft a special calculus and
    develop its proof techniques for each choice of cryptographic
    operation. Includes name restriction and variable restriction (as in the spi
    calculus), so fresh channels, nonces, and keys can be represented as new
    names. Attacks against protocols rely on equational properties.

    Example for one-way hash functions: Represent hash functions with a unary
    function symbol \textsf{h} (without any equational properties). The absence
    of an inverse for \textsf{h} models one-wayness. In comparison with spi
    calculus, the applied pi calculus permits a more uniform and versatile
    treatment of cryptographic functions (e.g., one-way hash functions,
    encryption/decryption, signatures, XOR), their variants, and their
    properties. \todo{Not clear to me why this is the case.} The spi calculus
    developed the idea that the context represents an active attacker, and
    equivalences capture authenticity and secrecy properties (same as here).
  \item Wysteria?~\cite{rastogi2014wysteria}
  \item Lambda auth?~\cite{miller2014authenticated}
  \item Fowler \etal~\cite{fowler2018session} develop a session typed
    programming language that is confluent. Only allows for fixed and two-party
    communications.
  \item Sequentiality and the $\pi$-calculus~\cite{berger2001sequentiality}. The
    authors developed a typed $\pi$-calculus that uses affineness and stateless
    replication to achieve deterministic computation. \todo{More details.}
  \item Secrecy types for a simulatable cryptographic
    library~\cite{laud2005secrecy}. They define a language for cryptographic
    protocols similar to the spi calculus and give it a semantics using the
    computational model of cryptography. They propose a type system for their
    language and show that if a protocol types, then it preserves the secrecy of
    messages given to it by users. The semantics of their calculus is
    deterministic. When complexity-theoretic security definitions are used,
    nondeterminism cannot be employed. The adversary is allowed to choose which
    thread handles the received message. 
  \item PPT process calculus for analysis of cryptographic
    protocols~\cite{mitchell2006probabilistic, lincoln1998probabilistic}. To
    avoid inconsistency between security and nondeterminism, messages are
    schedule probabilistically instead of nondeterministically. They prove that
    any process expression halts in polynomial time, and they define a form of
    asymptotic protocol equivalence that allows security properties to be
    expressed using observational equivalence, a standard relation that involves
    quantifying over all environments that might interact with the protocol. A
    limitation of deterministic or nondeterministic settings is the inability to
    analyze probabilistic protocols. Traditional nondeterministic scheduling
    means that an adversary has exponential computing power. With
    nondeterministic scheduling, an adversary can guess a $k$-bit key by
    concatenating $k$ bits. The combination of nondeterminism and bit-level
    representation of encryption keys renders any encryption function insecure.
  \item Composable crpytographic library with nested
    operations~\cite{backes2003composable}.
  \item Symmetric encryption in a simulatable Dolev-Yao style cryptographic
    library~\cite{backes2004symmetric}.
  \item Composition of cryptographic protocols in a p.p.t. process
    calculus~\cite{mateus2003composition}.
\end{enumerate}
\end{comment}
