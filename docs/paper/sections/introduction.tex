\section{Introduction}
\label{sec:introduction}

In cryptography, a proof of security in the simulation-based universal composability (UC)
framework is considered the gold standard for demonstrating that a protocol ``does its job
securely''~\cite{canetti2001universally}. In particular, a UC-secure protocol
enjoys the strongest notion of compositionality---it maintains all security
properties even when run \emph{concurrently} with arbitrary other protocol instances.
This is in contrast with weaker property-based notions that only guarantee security in a standalone
setting~\cite{lindell2014introduction} or under sequential
composition~\cite{goldreich1987play}.
%
\begin{comment}
In a nutshell, security proofs in UC follow the real/ideal
paradigm~\cite{goldreich1987play}. The security requirements of a given task are
defined as a program for a \emph{single trusted process} called an \emph{ideal
  functionality}, which runs in an imagined ideal world. This serves as a
specification of the desired security properties for a distributed protocol
achieving the task across \emph{many unstrusted processes}, which runs in the
real world. Roughly speaking, we say that a protocol $\pi$ \emph{emulates} an
ideal functionality $\mc{F}$ (i.e., it meets its specification) if every
adversarial behavior in the real world can also be exhibited in the ideal world.
\end{comment}
%
Thus, the benefit of using UC is
modularity---it supports analyzing complex protocols by composing simpler
building blocks.
However, the cost of UC is that security proofs tend to be quite
complicated.
We believe that applying a PL-style of systemization to UC can help
simplify its use, bring new clarity, and provide useful tooling.
We envision a future where modularity of cryptographic protocol composition
translates to modular implementation as well.

Reviewing prior efforts of applying PL techniques to cryptography,
we find they run up against challenges when importing the existing
body of UC theory.
Either they do not support
computational reasoning (which considers issues of probability and computational
complexity)~\cite{bohl2016symbolic}, do not support message-passing concurrency for distributed protocols~\cite{barthe2011computer},
or are too expressive (allow for expressing nondeterminism with no computational interpretation)~\cite{abadi1999calculus}.

Our observation is that these approaches diverge from UC at a low level: UC is
defined atop the underlying computational model of \emph{interactive Turing
  machines} (ITMs).
%
%Because the UC framework most exists ``on paper,'' protocol
%descriptions, specifications, and other proof components are
%written in a combination of prose and pseudocode. Not only does
%this make them error-prone and difficult to verify, but sometimes
%protocols have been subtly underspecified or ill-specified~\cite{camenisch2016universal}.
%
The significance of ITMs is that they have a clear computational interpretation,
so it is straightforward to relate execution traces to a probabilistic
polynomial time computation, as is necessary for cryptographic reduction proofs.
%  that breaks a computational
%hardness assumption with non-negligible
%probability~\cite{lindell2014introduction}.
%
The presence of (non-probabilistic) nondeterminism in alternative process calculi
would frustrate such reduction proofs.
ITMs avoid this issue by having a deterministic
(modulo random coin tosses), single-threaded execution semantics. That is,
processes pass control from one to another each time a message is sent so that
\emph{exactly one process is active at any given time}, and, moreover, this
order of activations is fully determined.

%Moreover, the order
%of activations is completely determined by the protocol definition, random coin
%flips taken by processes, and an explicit adversary process (for adversarial
%scheduling), rather than uncertainty (nondeterminism) built into the model
%itself.
%Security proofs require pervasive
%reasoning about probability and computational complexity, so modeling protocols
%at the low-level of Turing machines (while not particularly convenient) is often
%necessary for proofs to go through.

%\begin{enumerate}[leftmargin=*]
%  \item They support symbolic reasoning, but not computational reasoning (the
%    modus operandi in cryptography).
%  \item They are not well-suited to describing distributed protocols (the
%    intended target of UC).
%\end{enumerate}
%\begin{comment}
%For example, Symbolic UC~\cite{bohl2016symbolic} transports ideas from UC to the
%symbolic model of cryptography (as opposed to the computational model) to
%simplify analysis, but at the cost of weaker security
%guarantees~\cite{cortier2011survey}. On the other hand,
%EasyCrypt~\cite{barthe2011computer} embeds computational reasoning, but does not
%have a language for describing message-passing or concurrency. To faithfully
%model the UC framework, we would (at the very least) want to satisfy both of
%these criteria.
%, in which cryptographic operations
%are abstracted as term algebras and adversary capabilities are defined by
%deduction rules over these terms.
%Although analyzing security from this abstract
%vantage point can benefit from automated reasoning, security guarantees derived
%from symbolic analyses are not as strong as those derived from computational
%analyses~\cite{cortier2011survey}.
%\end{comment}

In this paper, we take up the challenge of faithfully capturing this
sequentiality by adapting ITMs to a subset of the $\pi$-calculus through an affine
type discipline. We call this process calculus the Interactive Lambda Calculus
(ILC), and we use it to build a concrete, executable implementation of the UC
framework, dubbed SaUCy.

%In sum, the goal of this work is twofold: first, to develop a foundational
%calculus for the purpose of systematizing UC (ILC), which importantly admits
%computational security proofs, and second, the concretization of the UC
%framework itself (SaUCy).

\subsection{Interactive Lambda Calculus}
Where have existing concurrency models fallen short with respect to ITMs? On the
one hand, some existing process calculi, such as the standard
$\pi$-calculus~\cite{milner1999communicating} or its cryptography-oriented
variants~\cite{abadi1999calculus, abadi2001mobile, lincoln1998probabilistic},
are not a good fit to ITMs, since they permit non-confluent reductions by design
(i.e., non-probabilistic nondeterminism).
%One could quite easily write programs in these languages that
%are not expressible as ITMs.
On the other hand, the various calculi that do enjoy confluence are
restrictive, only allowing for fixed or two-party
communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}.

ILC fills this gap by adapting ITMs to a subset of the $\pi$-calculus through an
affine typing discipline. In other words, \emph{well-typed ILC programs are
  expressible as ITMs}. To maintain that only one process is active (can write)
at any given time, processes implicitly pass around an affine ``write token'' by
virtue of where they perform read and write effects. Moreover, to maintain that
this order of activations is fully determined, read channels are an affine
resource, so each write operation corresponds with a single, unique read
operation. Together, these give ILC its central metatheoretic property of
confluence. The only nondeterminism in an ILC program is due to random coin
tosses taken by processes, which have a well-defined distribution. Additionally,
any apparent concurrency hazards, such as adversarial scheduling of messages in
an asynchronous network, are due to an explicit adversary process rather than
uncertainty built into the model itself.

\begin{comment}
To give a quick tour of ILC's type system, judgments have the form $\Delta ; \Gamma |- e
: A |> m$, where $\Delta$ is an affine typing context, $\Gamma$ is an intuitionistic typing
context, and $m \in \{\Wm, \Rm, \Vm\}$ is a mode (write, read, and value,
respectively). Importantly, it maintains these invariants:

\begin{itemize}[leftmargin=*]
\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
  ITM parlance) has a read end and a write end. The read end of the channel is
  protected against duplication by binding it in the affine context $\Delta$. This
  ensures that no nondeterminism arises at the receiving end of communications.

\item \emph{No parallel composition of write mode processes.} The typing rules
  do not allow parallel composition of two write mode processes ($\Wm ||
  \Wm$). This ensures that no nondeterminism arises at the sending end of
  communications.
\end{itemize}
\end{comment}

%We have implemented an ILC interpreter in Haskell. For economy of use, our
%implementation performs polymorphic type inference, bounded polymorphic mode
%inference, and affinity inference. Polymorphism on modes is bounded precisely
%due to our restriction on parallel write mode composition. Moreover, a
%consequence of any kind of mode polymorphism at all is that the modes of higher
%order functions can be dependent on the modes of its function arguments.

%\subsection{Super Amazing Universal ComposabilitY}
%\subsection{SaUCy}
%We use ILC to build a
%concrete implementation of the UC framework, we call SaUCy.
%SaUCy consists of an ILC implementation, and a definition
%of secure protocol emulation.
%stands % for Super Amazing Universal ComposabilitY. (This is in addition to ILC's
%standard prelude.)

%% Despite its widespread use in cryptography, the universal
%% composability framework remains an unwieldy tool.
%% Numerous variations of UC have been proposed to 
%% hitches~\cite{backes2007reactive, hofheinz2015gnuc, canetti2007universally,
%%   canetti2003universal} and to make it easier to
%% use~\cite{canetti2015simpler}.
%Unfortunately, the fact that UC and its variants
%exist on paper makes it difficult to keep track of the precise semantics of work
%done in this space, much less verify any security claims.


\subsection{Contributions}
\label{subsec:contributions}

To summarize, our main contributions are these:

\begin{itemize}[leftmargin=*]
  \item We design a foundational calculus for the purpose of systemizing UC
    called the Interactive Lambda Calculus, which exhibits confluence and is a faithful abstraction of ITMs.
  \item We use ILC to build a concrete, executable implementation of the UC
    framework called SaUCy.
  \item We then use SaUCy to port over a sampling of theory from UC
literature, including a composition theorem,
an instantiation proof of UC commitments~\cite{canetti2001commitments},
and an examination of a subtle definitional issue involving reentrant concurrency~\cite{camenisch2016universal}.
\end{itemize}
