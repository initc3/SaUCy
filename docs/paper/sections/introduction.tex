\section{Introduction}
\label{sec:introduction}

Demonstrating that a protocol ``does its job securely'' is an essential
component of cryptography, and a proof of security in the universal
composability (UC) framework is the gold standard for doing
so~\cite{canetti2001universally}. In particular, a UC-secure protocol enjoys the
strongest notion of compositionality---it maintains its security properties when
run \emph{concurrently} with arbitrary other protocols. This is in contrast with
weaker notions that only guarantee security in the standalone setting or under
sequential composition. Of course, another big payoff of using UC is
modularity---it allows us to analyze complex protocols from simpler building
blocks.

\begin{comment}
In a nutshell, security proofs in UC follow the real/ideal
paradigm~\cite{goldreich1987play}. The security requirements of a given task are
defined as a program for a \emph{single trusted process} called an \emph{ideal
  functionality}, which runs in an imagined ideal world. This serves as a
specification of the desired security properties for a distributed protocol
achieving the task across \emph{many unstrusted processes}, which runs in the
real world. Roughly speaking, we say that a protocol $\pi$ \emph{emulates} an
ideal functionality $\mc{F}$ (i.e., it meets its specification) if every
adversarial behavior in the real world can also be exhibited in the ideal world.
\end{comment}

While UC security proofs admit strong guarantees, this comes at the cost of
their formalization being quite complicated. And because the UC framework only
exists ``on paper,'' protocol descriptions, specifications, and other artifacts
for proving security are traditionally written in a combination of prose and
pseudocode. Not only does this make them error-prone and difficult to verify,
but it is sometimes the case that proven-secure protocols are subtly
underspecified or ill-specified, which leads to compositionality properties
being lost or complicated~\cite{camenisch2016universal}.

We believe that applying a PL-style of systemization to UC can help resolve
these issues. Unfortunately, we find that previous works in the vein of applying
PL-techniques to cryptography do not have quite the right abstraction for UC's
underlying computational model---\emph{interactive Turing machines (ITMs)}. Either
they do not support computational reasoning (which considers issues of
probability and computational complexity), are not well-suited to describing
distributed protocols (cannot express message-passing concurrency), or are too
expressive (allow describing a superset of ITMs).

Still, there are good reasons for having ITMs as the model of choice! In a
nutshell, sticking to ITMs ensures that whatever protocol we define has a
\emph{computational interpretation.} What, exactly, do we mean by this? We mean
it in the following cryptographic sense: We want to be able to prove security by
\emph{reduction}, i.e., by relating a bad execution trace to a computation that
breaks a computational hardness assumption with non-negligible probability.

The fact that we are dealing in a concurrent setting would threaten our desire of
having computational interpretations, since the presence of (non-probabilistic)
nondeterminism would greatly complicate probabilistic reasoning, but thankfully,
ITMs work around this by having a deterministic (modulo random coin tosses),
single-threaded execution semantics. That is, processes pass control from one to
another each time a message is sent so that exactly one process is active at any
given time, and, moreover, this order of activations is fully determined.

%Moreover, the order
%of activations is completely determined by the protocol definition, random coin
%flips taken by processes, and an explicit adversary process (for adversarial
%scheduling), rather than uncertainty (nondeterminism) built into the model
%itself.
%Security proofs require pervasive
%reasoning about probability and computational complexity, so modeling protocols
%at the low-level of Turing machines (while not particularly convenient) is often
%necessary for proofs to go through.

%\begin{enumerate}[leftmargin=*]
%  \item They support symbolic reasoning, but not computational reasoning (the
%    modus operandi in cryptography).
%  \item They are not well-suited to describing distributed protocols (the
%    intended target of UC).
%\end{enumerate}
%\begin{comment}
%For example, Symbolic UC~\cite{bohl2016symbolic} transports ideas from UC to the
%symbolic model of cryptography (as opposed to the computational model) to
%simplify analysis, but at the cost of weaker security
%guarantees~\cite{cortier2011survey}. On the other hand,
%EasyCrypt~\cite{barthe2011computer} embeds computational reasoning, but does not
%have a language for describing message-passing or concurrency. To faithfully
%model the UC framework, we would (at the very least) want to satisfy both of
%these criteria.
%, in which cryptographic operations
%are abstracted as term algebras and adversary capabilities are defined by
%deduction rules over these terms.
%Although analyzing security from this abstract
%vantage point can benefit from automated reasoning, security guarantees derived
%from symbolic analyses are not as strong as those derived from computational
%analyses~\cite{cortier2011survey}.
%\end{comment}

In this paper, we take up the challenge of faithfully capturing ITMs by adapting
them to a subset of the $\pi$-calculus through a type discipline. We call our
process calculus the Interactive Lambda Calculus (ILC), and we use it to build a
concrete, executable implementation of the UC framework, dubbed SaUCy.

%In sum, the goal of this work is twofold: first, to develop a foundational
%calculus for the purpose of systematizing UC (ILC), which importantly admits
%computational security proofs, and second, the concretization of the UC
%framework itself (SaUCy).

\subsection{Interactive Lambda Calculus}

So where have existing concurrency models fallen short?  It turns out that some
existing process calculi, such as the standard
$\pi$-calculus~\cite{milner1999communicating} or its cryptography-oriented
variants~\cite{abadi1999calculus, abadi2001mobile}, are not a good fit to ITMs,
since they allow for non-confluent reductions by design. This means that one
could quite easily write programs in these languages that are not expressible as
ITMs and hence, do not have computational interpretations. Other calculi may
enjoy confluence, but are overly restrictive, only allowing for fixed or
two-party
communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}.

We take up the challenge of faithfully capturing UC's computational model by
adapting ITMs to a subset of the $\pi$-calculus through its type system. In other
words, \emph{well-typed ILC programs are expressible as ITMs}.

To give a quick tour of ILC's type system, judgements have the form $\Delta ; \Gamma |- e
: A |> m$, where $\Delta$ is an affine typing context, $\Gamma$ is an intuitionistic typing
context, and $m \in \{\Wm, \Rm, \Vm\}$ is a mode (write, read, and value,
respectively). Importantly, it maintains these invariants:

\begin{itemize}[leftmargin=*]
\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
  ITM parlance) has a read end and a write end. The read end of the channel is
  protected against duplication by binding it in the affine context $\Delta$. This
  ensures that no non-determinism arises at the receiving end of communications.

\item \emph{No parallel composition of write mode processes.} The typing rules
  do not allow parallel composition of two write mode processes ($\Wm ||
  \Wm$). This ensures that no non-determinism arises at the sending end of
  communications.
\end{itemize}

Together, the effect of these is that the only non-determinism in an ILC program
is due to random coin flips taken by processes, which have a well-defined
distribution. So in addition to enjoying the usual properties of progress and
preservation, ILC is also confluent. This is essential to ensure that the
normalization of an ILC program has a computational interpretation, as is
necessary in cryptography reduction proofs. \todo{Why?}  This also guarantees
that any apparent concurrency hazards, such as adversarial scheduling of
messages in an asynchronous network, are due to an explicit adversary process
$\mc{A}$ rather than uncertainty built into the model itself.

We have implemented an ILC interpreter in Haskell. For economy of use, our
implementation performs polymorphic type inference, bounded polymorphic mode
inference, and affinity inference. Polymorphism on modes is bounded precisely
due to our restriction on parallel write mode composition. Moreover, a
consequence of any kind of mode polymorphism at all is that the modes of higher
order functions can be dependent on the modes of its function arguments.

\subsection{Super Amazing Universal ComposabilitY}

\todo{This section needs work.} Despite its widespread use, the universal
composability framework remains to be an unwieldy tool for cryptographers. Many
of the complications in UC are artifacts of its power and generality, so
naturally, numerous variants of UC have been proposed to address its
hitches~\cite{backes2007reactive, hofheinz2015gnuc, canetti2007universally,
  canetti2003universal} and to make it easier to
use~\cite{canetti2015simpler}. Unfortunately, the fact that UC and its variants
exist on paper makes it difficult to keep track of the precise semantics of work
done in this space, much less verify any security claims.

Prior to this work, UC has not had a sufficient abstraction for its underlying
computational model. To substantiate ILC's candidacy, we use ILC to build a
concrete implementation of the UC framework as a prelude called SaUCy, which
stands for Super Amazing Universal ComposabilitY. (This is in addition to ILC's
standard prelude.) We use SaUCy to elaborate an extended example, namely, UC
commitment schemes, and demonstrate that it exhibits previously known
impossibility results and workarounds~\cite{canetti2001commitments}. We are also
able transport theory from UC to SaUCy, such as protocol emulation.

\subsection{Contributions}
\label{subsec:contributions}

To summarize, our main contributions are these:

\begin{itemize}[leftmargin=*]
  \item We design a foundational calculus for the purpose of systematizing UC
    called the Interactive Lambda Calculus, which importantly admits
    computational security proofs.
  \item We use ILC to build a concrete, executable implementation of the UC
    framework called SaUCy.
  \item \todo{SaUCy validation.}
\end{itemize}
