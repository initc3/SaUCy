\section{Introduction}
\label{sec:introduction}

In cryptography, a proof of security in the universal composability (UC) model
is the gold standard for demonstrating that a protocol carries out a
cryptographic task securely~\cite{canetti2001universally}. More specifically, a
protocol that is UC-secure enjoys the strongest notion of compositionality---it
maintains its security properties when run concurrently with arbitrary other
protocols.

In a nutshell, security proofs in UC follow the real/ideal
paradigm~\cite{goldreich1987play}. The security requirements of a given task are
defined as a program for a \emph{single trusted process} called an \emph{ideal
  functionality}, which runs in an imagined ideal world. This serves as a
specification of the desired security properties for a distributed protocol
achieving the task across \emph{many unstrusted processes}, which runs in the
real world. Roughly speaking, we say that a protocol $\pi$ \emph{emulates} an
ideal functionality $\mc{F}$ (i.e., it meets its specification) if every
adversarial behavior in the real world can also be exhibited in the ideal world.

While UC security proofs admit strong guarantees, this comes at the cost of
their formalization being quite complicated. Moreover, because the UC framework
only exists on paper, protocol descriptions, ideal functionalities, and other
proof artifacts for proving emulation are traditionally written in a combination
of prose and pseudocode, which makes them error-prone and difficult to
verify. Consequently, although the UC framework is widely used for on-paper
proofs, its modularity benefits have not been enjoyed in practical
implementations.

Prior work on systematizing UC, such as Symoblic UC~\cite{bohl2016symbolic},
focus on transporting its ideas to the symbolic model of cryptography, in which
cryptographic operations are abstracted as term algebras and adversary
capabilities are defined by deduction rules over these terms. Although this
abstract vantage point leads to simpler security proofs that can be amenable to
automated reasoning, security guarantees derived from symbolic analyses are not
as strong as those from computational analyses considered in UC and in
cryptography more broadly~\cite{cortier2011survey}.

In this paper, we lay the groundwork for building a concrete implementation of
the UC framework called SaUCy in the form of a process calculus called the
Interactive Lambda Calculus (ILC). ILC represents the first language-based
approach to capturing the computational model underlying UC---interactive Turing
machines (ITMs). In the ITM model, execution is essentially
single-threaded. Processes pass control from one to another each time a message
is sent so that exactly one process is active at a given time. And this choice
is for good reason! It is necessary for computational security proofs
(specifically, cryptographic reductions) to go through.

In sum, the goal of this work is twofold: to develop a foundational calculus for
the purpose of systematizing UC (ILC), which importantly admits computational
security proofs, and the concretization of the UC framework itself (SaUCy).

\subsection{Interactive Lambda Calculus}

So where have existing concurrency models fallen short?  It turns out that some
existing process calculi, such as the standard
$\pi$-calculus~\cite{milner1999communicating} or its cryptography-oriented
variants~\cite{abadi1999calculus, abadi2001mobile}, are not a good fit to ITMs,
since they allow for non-confluent reductions by design. This means that one
could quite easily write programs in these languages are not expressible in ITMs
and hence, do not have computational interpretations. Other calculi enjoy
confluence, but are overly restrictive, only allowing for fixed or two-party
communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}.

We take up the challenge of faithfully capturing UC's computational model by
adapting ITMs to a subset of the $\pi$-calculus through its type system. In other
words, \emph{well-typed ILC programs are expressible as ITMs}.

To give a quick tour of ILC's type system, judgements have the form $\Delta ; \Gamma |- e
: A |> m$, where $\Delta$ is a linear typing context and $m \in \{\Wm, \Rm, \Vm\}$ is a
mode (write, read, and value, respectively), and it maintains the following two
invariants:

\begin{enumerate}[leftmargin=*]
\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
  ITM parlance) has a read end and a write end. The read end of the channel is
  protected against duplication by binding it in the linear context $\Delta$. This
  ensures that no non-determinism arises at the receiving end of communications.

\item \emph{No parallel composition of write mode processes.} The typing rules
  do not allow parallel composition of two write mode processes ($\Wm ||
  \Wm$). This ensures that no non-determinism arises at the sending end of
  communications.
\end{enumerate}

Taken together, the effect of these is that the only non-determinism in an ILC
program is due to random coin flips taken by processes, which have a
well-defined distribution. So in addition to enjoying the usual properties of
progress and preservation, ILC is also confluent in the absence of coin
flips. This is essential to ensure that the normalization of an ILC program has
a computational interpretation, as is necessary in cryptography reduction
proofs. This also guarantees that any apparent concurrency hazards, such as
adversarial scheduling of messages in an asynchronous network, are due to an
explicit adversary process $\mc{A}$ rather than uncertainty built into the model
itself.

We have implemented an ILC interpreter in Haskell. For economy of use, our
implementation performs polymorphic type inference, bounded polymorphic mode
inference, and linearity inference. Polymorphism on modes is bounded precisely
due to our restriction on parallel write mode composition. Moreover, a
consequence of any mode polymorphism at all is that the mode of a higher order
function can be dependent on the mode(s) of its function argument(s).

\subsection{Super Amazing Universal ComposabilitY}

Despite its widespread use, the universal composability framework remains to be
an unwieldy tool for cryptographers. Many of the complications in UC are
artifacts of its power and generality, so naturally, numerous variants of UC
have been proposed to address its hitches~\cite{backes2007reactive,
  hofheinz2015gnuc, canetti2007universally, canetti2003universal} and to make it
easier to use~\cite{canetti2015simpler}. Unfortunately, the fact that UC and its
variants exist on paper makes it difficult to keep track of the precise
semantics of work done in this space, much less verify any security claims.

Prior to this work, UC has not had a sufficient abstraction for its underlying
computational model. To substantiate ILC's candidacy, we use ILC to build a
concrete implementation of the UC framework as a UC-specific prelude called
SaUCy. (This is in addition to ILC's standard prelude.) We use SaUCy to
elaborate an extended example, namely, UC commitment schemes, and demonstrate
that it exhibits previously known impossibility results and
workarounds~\cite{canetti2001commitments}. We are also able transport theory
from UC to SaUCy, such as protocol emulation.

%One can imagine giving a function that composes two processes in
%parallel the general mode $\forall m \in \{\Rm, \Vm\}.\, m || m$, as opposed to 

%Our implementation facilitates economy
%of use with support for an interactive environment, an Emacs major mode, pattern
%matching (including guards), custom data declarations, and type inference. In
%particular, our implementation performs full polymorphic type inference for
%intuitionistic types, bounded polymorphic inference for modes, and linearity
%inference.

\begin{comment}

%In particular, the invariants maintained by the type system ensure that ILC
%programs are confluent, so any non-determinism in protocols is due to random
%coinflips taken by processes, which have a well-defined distribution. This is
%essential to ensure that the normalization of an ILC program has a computational
%interpretation, as is necessary in cryptographic reduction proofs. While some
%existing process calculi enjoy confluence properties, they are insufficient as a
%foundation for UC, because they either rule out nondeterminism entirely or are
%restricted to two-party
%communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}. On
%the other hand, ILC achieves confluence through its type system, which has the
%judgement form $\Delta ; \Gamma |- e : A |> m$, where $\Delta$ is a linear typing context and
%$m \in \{\Wm, \Rm, \Vm\}$ is a mode (write, read, and value, respectively), and
%boils down to the following two invariants:
%\begin{enumerate}[leftmargin=*]
%\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
%  ITM parlance) has a read end and a write end. The read end of the channel is
%  protected against duplication by binding it in the linear context $\Delta$. This
%  ensures that no non-determinism arises at the receiving end of communications.
%
%\item \emph{No parallel composition of write mode processes.} The typing rules
%  do not allow parallel composition of two write mode processes ($\Wm ||
%  \Wm$). This ensures that no non-determinism arises at the sending end of
%  communications.
  %\end{enumerate}
  
The success of blockchains and cryptocurrencies have raised interest in building
secure software systems that combine consensus protocols~\cite{miller2016honey},
zero-knowledge proofs~\cite{kosba2016hawk}, multiparty
computation~\cite{bentov2017instantaneous}, and other advanced techniques from
distributed computing and cryptography.  However, these primitives are known to
be error-prone and difficult to compose securely.  To the average developer,
reasoning about asynchronous, distributed, and adversarial deployment
environments is unnatural. On top of this, the security of a software system is
generally a whole-system property, but vulnerabilities often arise from
misunderstandings and mismatches as components are
integrated~\cite{chong2016report}.

Our solution is to develop a module system, \saucy, that will simplify the task
of composing distributed protocols and cryptographic primitives.  The novel
design idea of \saucy is to include with each module a rich behavioral
specification in the form of an \emph{ideal functionality}, which serves as a
self-contained specification of all desired security and liveness properties.
This idea is rooted in the theory of \emph{universal composability}
(UC)~\cite{canetti2001universally}, which is widely used in cryptography for
on-paper proofs, but has not yet been adapted for software engineering.  Based
on our prior experience providing formal specifications for smart contract and
blockchain protocols~\cite{bentov2017instantaneous, kosba2016hawk,
  miller2017sprites}, ideal functionalities are well-suited for modular design
of complex security-oriented applications for several reasons:

\begin{enumerate}
\item The UC framework is an established standard for modeling distributed and
  cryptographic protocols, so we can draw on existing literature for ideal
  functionality models.
\item Ideal functionalities are executable specifications, so they are amenable
  to property-based testing and machine-checkable proofs.
\item UC provides the strongest notion of security under concurrent
  composition. When we substitute an ideal functionality for a distributed
  protocol that realizes it, all the properties of the ideal functionality are
  preserved. Hence a developer's understanding of the ideal functionality
  carries over to the distributed implementation.
\end{enumerate}

\subsection{Our Approach}
\label{subsec:approach}

To reap the expected benefits of the \saucy module system, in this work, we will
develop infrastructure to help authors write, test, and verify distributed
protocol and cryptographic primitives.  This work will take place over three
main tasks: The first task focuses on designing a new high-level language called
ILC for expressing protocols and implementing the UC framework, the second
task focuses on developing testing techniques for detecting security and
liveness violations in the presence of Byzantine failures, and the third task
focuses on building a verification tool for mechanizing security and liveness
proofs in the UC framework.\smallskip

\subsection{Organization}
\label{subsec:org}

This paper is organized as follows. Section~\ref{sec:background} provides an
overview of the UC framework and potential applications.
Section~\ref{sec:challenges} highlights several challenges in using
UC. Section~\ref{sec:ilc} describes the design of our programming language
Interactive Lambda Calculus (ILC). Section~\ref{sec:session} describes an
extension of ILC with session types that will enable a form of verification for
ILC programs. Section~\ref{sec:testing} details our plan to develop new
techniques for testing security and liveness of ILC procotols in the presence of
Byzantine failures. Section~\ref{sec:verification} details our plan to develop a
proof assistant for mechanizing UC proofs.
\end{comment}
