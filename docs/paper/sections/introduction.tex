\section{Introduction}
\label{sec:introduction}

In cryptography, a proof of security in the universal composability (UC)
framework is the gold standard for demonstrating that a protocol carries out a
cryptographic task securely~\cite{canetti2001universally}. In particular, a
UC-secure protocol enjoys the strongest notion of compositionality---it maintains
its security properties when run \emph{concurrently} with arbitrary other
protocols. This is in contrast with weaker notions, which only guarantee
security in the standalone setting or under sequential composition. Of course,
the big payoff of this is that it allows us to design and analyze complex
protocols in a modular fashion from simpler building blocks.

\begin{comment}
In a nutshell, security proofs in UC follow the real/ideal
paradigm~\cite{goldreich1987play}. The security requirements of a given task are
defined as a program for a \emph{single trusted process} called an \emph{ideal
  functionality}, which runs in an imagined ideal world. This serves as a
specification of the desired security properties for a distributed protocol
achieving the task across \emph{many unstrusted processes}, which runs in the
real world. Roughly speaking, we say that a protocol $\pi$ \emph{emulates} an
ideal functionality $\mc{F}$ (i.e., it meets its specification) if every
adversarial behavior in the real world can also be exhibited in the ideal world.
\end{comment}

While UC security proofs admit strong guarantees, this comes at the cost of
their formalization being quite complicated. And because the UC framework only
exists ``on paper,'' protocol descriptions, specifications, and other proof
artifacts for proving security are traditionally written in a combination of
prose and pseudocode, which makes them error-prone and difficult to
verify. Moreover, it is sometimes the case that specifications of proven-secure
protocols are underspecified or ill-specified, and so compositionality
properties may be lost~\cite{camenisch2016universal}.

We believe that applying a PL-style of systemization to UC can help to resolve
these issues. While this idea is not new, we find that previous work falls short
of appropriately abstracting the underlying computational model in
UC---interactive Turing machines (ITMs).

In the ITM model, execution is essentially single-threaded: Processes pass
control from one to another each time a message is sent so that exactly one
process is active at a given time. And this choice is for good reason! It is
necessary for computational security proofs (specifically, cryptographic
reductions) to go through.

\begin{enumerate}[leftmargin=*]
  \item They support symbolic reasoning, but not computational reasoning (the
    modus operandi in cryptography).
  \item They are not well-suited to describing distributed protocols (the
    intended target of UC).
\end{enumerate}
\begin{comment}
For example, Symbolic UC~\cite{bohl2016symbolic} transports ideas from UC to the
symbolic model of cryptography (as opposed to the computational model) to
simplify analysis, but at the cost of weaker security
guarantees~\cite{cortier2011survey}. On the other hand,
EasyCrypt~\cite{barthe2011computer} embeds computational reasoning, but does not
have a language for describing message-passing or concurrency. To faithfully
model the UC framework, we would (at the very least) want to satisfy both of
these criteria.
%, in which cryptographic operations
%are abstracted as term algebras and adversary capabilities are defined by
%deduction rules over these terms.
%Although analyzing security from this abstract
%vantage point can benefit from automated reasoning, security guarantees derived
%from symbolic analyses are not as strong as those derived from computational
%analyses~\cite{cortier2011survey}.
\end{comment}

\todo{Fix rest of intro.} In this paper, we lay the foundation for building a
concrete, executable implementation of the UC framework (dubbed SaUCy) in the
form of a process calculus called the Interactive Lambda Calculus (ILC). ILC
represents the first language-based approach to capturing the computational
model underlying UC---interactive Turing machines (ITMs). In the ITM model,
execution is essentially single-threaded: Processes pass control from one to
another each time a message is sent so that exactly one process is active at a
given time. And this choice is for good reason! It is necessary for
computational security proofs (specifically, cryptographic reductions) to go
through. By staying true to the ITM model, we can satisfy both of the mentioned
criteria at once.

%In sum, the goal of this work is twofold: first, to develop a foundational
%calculus for the purpose of systematizing UC (ILC), which importantly admits
%computational security proofs, and second, the concretization of the UC
%framework itself (SaUCy).

\subsection{Interactive Lambda Calculus}

So where have existing concurrency models fallen short?  It turns out that some
existing process calculi, such as the standard
$\pi$-calculus~\cite{milner1999communicating} or its cryptography-oriented
variants~\cite{abadi1999calculus, abadi2001mobile}, are not a good fit to ITMs,
since they allow for non-confluent reductions by design. This means that one
could quite easily write programs in these languages that are not expressible as
ITMs and hence, do not have computational interpretations. Other calculi may
enjoy confluence, but are overly restrictive, only allowing for fixed or
two-party
communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}. \todo{Explain
  why confluence leads to computational interpretations.}

We take up the challenge of faithfully capturing UC's computational model by
adapting ITMs to a subset of the $\pi$-calculus through its type system. In other
words, \emph{well-typed ILC programs are expressible as ITMs}.

To give a quick tour of ILC's type system, judgements have the form $\Delta ; \Gamma |- e
: A |> m$, where $\Delta$ is an affine typing context, $\Gamma$ is an intuitionistic typing
context, and $m \in \{\Wm, \Rm, \Vm\}$ is a mode (write, read, and value,
respectively). Importantly, it maintains these invariants:

\begin{itemize}[leftmargin=*]
\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
  ITM parlance) has a read end and a write end. The read end of the channel is
  protected against duplication by binding it in the affine context $\Delta$. This
  ensures that no non-determinism arises at the receiving end of communications.

\item \emph{No parallel composition of write mode processes.} The typing rules
  do not allow parallel composition of two write mode processes ($\Wm ||
  \Wm$). This ensures that no non-determinism arises at the sending end of
  communications.
\end{itemize}

Together, the effect of these is that the only non-determinism in an ILC program
is due to random coin flips taken by processes, which have a well-defined
distribution. So in addition to enjoying the usual properties of progress and
preservation, ILC is also confluent in the absence of coin flips. This is
essential to ensure that the normalization of an ILC program has a computational
interpretation, as is necessary in cryptography reduction proofs. \todo{Why?}
This also guarantees that any apparent concurrency hazards, such as adversarial
scheduling of messages in an asynchronous network, are due to an explicit
adversary process $\mc{A}$ rather than uncertainty built into the model itself.

We have implemented an ILC interpreter in Haskell. For economy of use, our
implementation performs polymorphic type inference, bounded polymorphic mode
inference, and affinity inference. Polymorphism on modes is bounded precisely
due to our restriction on parallel write mode composition. Moreover, a
consequence of any kind of mode polymorphism at all is that the modes of higher
order functions can be dependent on the modes of its function arguments.

\subsection{Super Amazing Universal ComposabilitY}

\todo{This section needs work.} Despite its widespread use, the universal
composability framework remains to be an unwieldy tool for cryptographers. Many
of the complications in UC are artifacts of its power and generality, so
naturally, numerous variants of UC have been proposed to address its
hitches~\cite{backes2007reactive, hofheinz2015gnuc, canetti2007universally,
  canetti2003universal} and to make it easier to
use~\cite{canetti2015simpler}. Unfortunately, the fact that UC and its variants
exist on paper makes it difficult to keep track of the precise semantics of work
done in this space, much less verify any security claims.

Prior to this work, UC has not had a sufficient abstraction for its underlying
computational model. To substantiate ILC's candidacy, we use ILC to build a
concrete implementation of the UC framework as a prelude called SaUCy, which
stands for Super Amazing Universal ComposabilitY. (This is in addition to ILC's
standard prelude.) We use SaUCy to elaborate an extended example, namely, UC
commitment schemes, and demonstrate that it exhibits previously known
impossibility results and workarounds~\cite{canetti2001commitments}. We are also
able transport theory from UC to SaUCy, such as protocol emulation.

\subsection{Contributions}
\label{subsec:contributions}

To summarize, our main contributions are these:

\begin{itemize}[leftmargin=*]
  \item We design a foundational calculus for the purpose of systematizing UC
    called the Interactive Lambda Calculus, which importantly admits
    computational security proofs.
  \item We use ILC to build a concrete, executable implementation of the UC
    framework called SaUCy.
  \item \todo{SaUCy validation.}
\end{itemize}
