\section{Introduction}
\label{sec:introduction}

In cryptography, a proof of security in the universal composability (UC)
framework is the gold standard for demonstrating that a protocol ``does its job
securely''~\cite{canetti2001universally}. In particular, a UC-secure protocol
enjoys the strongest notion of compositionality---it maintains its security
properties when run \emph{concurrently} with arbitrary other protocols. This is
in contrast with weaker notions that only guarantee security in the standalone
setting~\cite{lindell2014introduction} or under sequential
composition~\cite{goldreich1987play}. Of course, the big payoff of using UC is
its modularity benefits---it allows analyzing complex protocols from simpler
building blocks.

\begin{comment}
In a nutshell, security proofs in UC follow the real/ideal
paradigm~\cite{goldreich1987play}. The security requirements of a given task are
defined as a program for a \emph{single trusted process} called an \emph{ideal
  functionality}, which runs in an imagined ideal world. This serves as a
specification of the desired security properties for a distributed protocol
achieving the task across \emph{many unstrusted processes}, which runs in the
real world. Roughly speaking, we say that a protocol $\pi$ \emph{emulates} an
ideal functionality $\mc{F}$ (i.e., it meets its specification) if every
adversarial behavior in the real world can also be exhibited in the ideal world.
\end{comment}

While UC security proofs admit strong guarantees, this comes at the cost of
their formalization being quite complicated. And because the UC framework only
exists ``on paper,'' protocol descriptions, specifications, and other artifacts
for proving security are traditionally written in a combination of prose and
pseudocode. Not only does this make them error-prone and difficult to verify,
but it is sometimes the case that proven-secure protocols are subtly
underspecified or ill-specified, which leads to compositionality properties
being lost or complicated~\cite{camenisch2016universal}.

We believe that applying a PL-style of systemization to UC can help resolve
these issues. Unfortunately, we find that previous works in the vein of applying
PL-techniques to cryptography are not a good fit to capturing \emph{interactive
  Turing machines} (ITMs), the computational model underlying UC.\footnote{One
  can think of interactive Turing machines as normal Turing machines that can
  write on each other's tapes.} Either they do not support computational
reasoning (which considers issues of probability and computational complexity),
are not well-suited to describing distributed protocols (cannot express
message-passing concurrency), or are too expressive (allow describing a superset
of ITMs).

Still, there are compelling reasons for sticking with ITMs as the model of
choice. Most importantly, it ensures that whatever protocol we define in the
model has a \emph{computational interpretation.} What, exactly, do we mean by
this? We mean it in the following cryptographic sense: We should be able to
prove its security by \emph{reduction}, i.e., by relating a bad execution trace
to a computation that breaks a computational hardness assumption with
non-negligible probability~\cite{lindell2014introduction}.

Were it not for ITMs, the presence of (non-probabilistic) nondeterminism due to
concurrency would severely complicate the necessary probabilistic reasoning for
reduction proofs. However, ITMs avoid this issue by having a deterministic
(modulo random coin tosses), single-threaded execution semantics. That is,
processes pass control from one to another each time a message is sent so that
\emph{exactly one process is active at any given time}, and, moreover, this
order of activations is fully determined.

%Moreover, the order
%of activations is completely determined by the protocol definition, random coin
%flips taken by processes, and an explicit adversary process (for adversarial
%scheduling), rather than uncertainty (nondeterminism) built into the model
%itself.
%Security proofs require pervasive
%reasoning about probability and computational complexity, so modeling protocols
%at the low-level of Turing machines (while not particularly convenient) is often
%necessary for proofs to go through.

%\begin{enumerate}[leftmargin=*]
%  \item They support symbolic reasoning, but not computational reasoning (the
%    modus operandi in cryptography).
%  \item They are not well-suited to describing distributed protocols (the
%    intended target of UC).
%\end{enumerate}
%\begin{comment}
%For example, Symbolic UC~\cite{bohl2016symbolic} transports ideas from UC to the
%symbolic model of cryptography (as opposed to the computational model) to
%simplify analysis, but at the cost of weaker security
%guarantees~\cite{cortier2011survey}. On the other hand,
%EasyCrypt~\cite{barthe2011computer} embeds computational reasoning, but does not
%have a language for describing message-passing or concurrency. To faithfully
%model the UC framework, we would (at the very least) want to satisfy both of
%these criteria.
%, in which cryptographic operations
%are abstracted as term algebras and adversary capabilities are defined by
%deduction rules over these terms.
%Although analyzing security from this abstract
%vantage point can benefit from automated reasoning, security guarantees derived
%from symbolic analyses are not as strong as those derived from computational
%analyses~\cite{cortier2011survey}.
%\end{comment}

In this paper, we take up the challenge of faithfully capturing this
sequentiality by adapting ITMs to a subset of the $\pi$-calculus through a type
discipline. We call this process calculus the Interactive Lambda Calculus (ILC),
and we use it to build a concrete, executable implementation of the UC
framework, dubbed SaUCy.

%In sum, the goal of this work is twofold: first, to develop a foundational
%calculus for the purpose of systematizing UC (ILC), which importantly admits
%computational security proofs, and second, the concretization of the UC
%framework itself (SaUCy).

\subsection{Interactive Lambda Calculus}

With regard to ITMs, where have existing concurrency models fallen short? On the
one hand, some existing process calculi, such as the standard
$\pi$-calculus~\cite{milner1999communicating} or its cryptography-oriented
variants~\cite{abadi1999calculus, abadi2001mobile, lincoln1998probabilistic},
are not a good fit to ITMs, since they permit non-confluent reductions by design
(i.e., non-probabilistic nondeterminism).
%One could quite easily write programs in these languages that
%are not expressible as ITMs.
On the other hand, the various calculi that do enjoy confluence are overly
restrictive, only allowing for fixed or two-party
communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}.

ILC fills this gap by adapting ITMs to a subset of the $\pi$-calculus through its
type system. In other words, \emph{well-typed ILC programs are expressible as
  ITMs}. To give a brief summary, ILC's type system restricts the dataflow of
read channels (via affine typing) and restricts the composition of write effects
(forbidding write-mode parallel processes), with the net effect being
confluence. Therefore, the only nondeterminism in an ILC program is due to
random coin tosses taken by processes, which have a well-defined distribution,
and any apparent concurrency hazards, such as adversarial scheduling of messages
in an asynchronous network, are due to an explicit adversary process rather than
uncertainty built into the model itself.

\begin{comment}
To give a quick tour of ILC's type system, judgements have the form $\Delta ; \Gamma |- e
: A |> m$, where $\Delta$ is an affine typing context, $\Gamma$ is an intuitionistic typing
context, and $m \in \{\Wm, \Rm, \Vm\}$ is a mode (write, read, and value,
respectively). Importantly, it maintains these invariants:

\begin{itemize}[leftmargin=*]
\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
  ITM parlance) has a read end and a write end. The read end of the channel is
  protected against duplication by binding it in the affine context $\Delta$. This
  ensures that no nondeterminism arises at the receiving end of communications.

\item \emph{No parallel composition of write mode processes.} The typing rules
  do not allow parallel composition of two write mode processes ($\Wm ||
  \Wm$). This ensures that no nondeterminism arises at the sending end of
  communications.
\end{itemize}
\end{comment}

%We have implemented an ILC interpreter in Haskell. For economy of use, our
%implementation performs polymorphic type inference, bounded polymorphic mode
%inference, and affinity inference. Polymorphism on modes is bounded precisely
%due to our restriction on parallel write mode composition. Moreover, a
%consequence of any kind of mode polymorphism at all is that the modes of higher
%order functions can be dependent on the modes of its function arguments.

\subsection{Super Amazing Universal ComposabilitY}

\todo{This section needs work.} Despite its widespread use, the universal
composability framework remains to be an unwieldy tool for cryptographers. Many
of the complications in UC are artifacts of its power and generality, so
naturally, numerous variants of UC have been proposed to address its
hitches~\cite{backes2007reactive, hofheinz2015gnuc, canetti2007universally,
  canetti2003universal} and to make it easier to
use~\cite{canetti2015simpler}. Unfortunately, the fact that UC and its variants
exist on paper makes it difficult to keep track of the precise semantics of work
done in this space, much less verify any security claims.

Prior to this work, UC has not had a sufficient abstraction for its underlying
computational model. To substantiate ILC's candidacy, we use ILC to build a
concrete implementation of the UC framework as a prelude called SaUCy, which
stands for Super Amazing Universal ComposabilitY. (This is in addition to ILC's
standard prelude.) We use SaUCy to elaborate an extended example, namely, UC
commitment schemes, and demonstrate that it exhibits previously known
impossibility results and workarounds~\cite{canetti2001commitments}. We are also
able transport theory from UC to SaUCy, such as protocol emulation.

\subsection{Contributions}
\label{subsec:contributions}

To summarize, our main contributions are these:

\begin{itemize}[leftmargin=*]
  \item We design a foundational calculus for the purpose of systematizing UC
    called the Interactive Lambda Calculus, which importantly admits
    computational security proofs.
  \item We use ILC to build a concrete, executable implementation of the UC
    framework called SaUCy.
  \item \todo{SaUCy validation.}
\end{itemize}
