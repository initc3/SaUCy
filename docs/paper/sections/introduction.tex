\section{Introduction}
\label{sec:introduction}

In cryptography, a proof of security in the universal composability (UC) model
is the gold standard for demonstrating that a protocol carries out a
cryptographic task securely~\cite{canetti2001universally}. More specifically, a
protocol that is UC-secure enjoys the strongest notion of compositionality---it
maintains its security properties when run concurrently with arbitrary other
protocols.\todo{Modularity is another benefit.}

In a nutshell, security proofs in UC follow the real/ideal
paradigm~\cite{goldreich1987play}. The security requirements of a given task are
defined as a program for a \emph{single trusted process} called an \emph{ideal
  functionality}, which runs in an imagined ideal world. This serves as a
specification of the desired security properties for a distributed protocol
achieving the task across \emph{many unstrusted processes}, which runs in the
real world. Roughly speaking, we say that a protocol $\pi$ \emph{emulates} an
ideal functionality $\mc{F}$ (i.e., it meets its specification) if every
adversarial behavior in the real world can also be exhibited in the ideal world.

While UC security proofs admit strong guarantees, this comes at the cost of
their formalization being quite complicated. Moreover, because the UC framework
only exists on paper, protocol descriptions, ideal functionalities, and other
proof artifacts for proving emulation are traditionally written in a combination
of prose and pseudocode, which makes them error-prone and difficult to
verify. Consequently, although the UC framework is widely used for on-paper
proofs, its modularity benefits have not been enjoyed in practical
implementations.

Prior work on systematizing UC, such as Symoblic UC~\cite{bohl2016symbolic},
focus on transporting its ideas to the symbolic model of cryptography, in which
cryptographic operations are abstracted as term algebras and adversary
capabilities are defined by deduction rules over these terms. Although this
abstract vantage point leads to simpler security proofs that can be amenable to
automated reasoning, security guarantees derived from symbolic analyses are not
as strong as those from computational analyses considered in UC and in
cryptography more broadly~\cite{cortier2011survey}. \todo{EasyCrypt is also
  worth mentioning. EasyCrypt is a faithful embedding of computational
  cryptography, but language is not well-suited to express distributed
  protocols.}

In this paper, we lay the groundwork for building a concrete, executable
implementation of the UC framework called SaUCy in the form of a process
calculus called the Interactive Lambda Calculus (ILC). ILC represents the first
language-based approach to capturing the computational model underlying
UC---interactive Turing machines (ITMs). In the ITM model, execution is
essentially single-threaded. Processes pass control from one to another each
time a message is sent so that exactly one process is active at a given
time. And this choice is for good reason! It is necessary for computational
security proofs (specifically, cryptographic reductions) to go through.

In sum, the goal of this work is twofold: first, to develop a foundational
calculus for the purpose of systematizing UC (ILC), which importantly admits
computational security proofs, and second, the concretization of the UC
framework itself (SaUCy).

\subsection{Interactive Lambda Calculus}

So where have existing concurrency models fallen short?  It turns out that some
existing process calculi, such as the standard
$\pi$-calculus~\cite{milner1999communicating} or its cryptography-oriented
variants~\cite{abadi1999calculus, abadi2001mobile}, are not a good fit to ITMs,
since they allow for non-confluent reductions by design. This means that one
could quite easily write programs in these languages that are not expressible in
ITMs and hence, do not have computational interpretations. Other calculi may
enjoy confluence, but are overly restrictive, only allowing for fixed or
two-party
communications~\cite{kobayashi1999linearity,bohl2016symbolic,fowler2018session}. \todo{Explain
  why confluence leads to computational interpretations.}

We take up the challenge of faithfully capturing UC's computational model by
adapting ITMs to a subset of the $\pi$-calculus through its type system. In other
words, \emph{well-typed ILC programs are expressible as ITMs}.

To give a quick tour of ILC's type system, judgements have the form $\Delta ; \Gamma |- e
: A |> m$, where $\Delta$ is an affine typing context, $\Gamma$ is an intuitionistic typing
context, and $m \in \{\Wm, \Rm, \Vm\}$ is a mode (write, read, and value,
respectively). Importantly, it maintains these invariants:

\begin{itemize}[leftmargin=*]
\item \emph{No duplication of read channel ends.} Each channel (or ``tape'' in
  ITM parlance) has a read end and a write end. The read end of the channel is
  protected against duplication by binding it in the affine context $\Delta$. This
  ensures that no non-determinism arises at the receiving end of communications.

\item \emph{No parallel composition of write mode processes.} The typing rules
  do not allow parallel composition of two write mode processes ($\Wm ||
  \Wm$). This ensures that no non-determinism arises at the sending end of
  communications.
\end{itemize}

Together, the effect of these is that the only non-determinism in an ILC program
is due to random coin flips taken by processes, which have a well-defined
distribution. So in addition to enjoying the usual properties of progress and
preservation, ILC is also confluent in the absence of coin flips. This is
essential to ensure that the normalization of an ILC program has a computational
interpretation, as is necessary in cryptography reduction proofs. \todo{Why?}
This also guarantees that any apparent concurrency hazards, such as adversarial
scheduling of messages in an asynchronous network, are due to an explicit
adversary process $\mc{A}$ rather than uncertainty built into the model itself.

We have implemented an ILC interpreter in Haskell. For economy of use, our
implementation performs polymorphic type inference, bounded polymorphic mode
inference, and \todo{some} affinity inference. Polymorphism on modes is bounded
precisely due to our restriction on parallel write mode composition. Moreover, a
consequence of any kind of mode polymorphism at all is that the modes of higher
order functions can be dependent on the modes of its function arguments.

\subsection{Super Amazing Universal ComposabilitY}

\todo{This section needs work.} Despite its widespread use, the universal
composability framework remains to be an unwieldy tool for cryptographers. Many
of the complications in UC are artifacts of its power and generality, so
naturally, numerous variants of UC have been proposed to address its
hitches~\cite{backes2007reactive, hofheinz2015gnuc, canetti2007universally,
  canetti2003universal} and to make it easier to
use~\cite{canetti2015simpler}. Unfortunately, the fact that UC and its variants
exist on paper makes it difficult to keep track of the precise semantics of work
done in this space, much less verify any security claims.

Prior to this work, UC has not had a sufficient abstraction for its underlying
computational model. To substantiate ILC's candidacy, we use ILC to build a
concrete implementation of the UC framework as a prelude called SaUCy, which
stands for Super Amazing Universal ComposabilitY. (This is in addition to ILC's
standard prelude.) We use SaUCy to elaborate an extended example, namely, UC
commitment schemes, and demonstrate that it exhibits previously known
impossibility results and workarounds~\cite{canetti2001commitments}. We are also
able transport theory from UC to SaUCy, such as protocol emulation.
